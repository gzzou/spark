/**
 <dependency>
 <groupId>com.google.guava</groupId>
 <artifactId>guava</artifactId>
 <version>11.0.2</version>
 </dependency>
 <dependency>
 <groupId>com.googlecode.matrix-toolkits-java</groupId>
 <artifactId>mtj</artifactId>
 <version>1.0.1</version>
 </dependency>

 */

package com.zhe800.toona.lda.serving;

import java.io.File;
import java.io.IOException;
import java.util.Iterator;
import java.util.List;
import java.util.Random;

import com.google.common.base.Charsets;
import com.google.common.base.Preconditions;
import com.google.common.io.Files;
import no.uib.cipr.matrix.DenseVector;
import no.uib.cipr.matrix.Vector.Norm;
import no.uib.cipr.matrix.VectorEntry;
import no.uib.cipr.matrix.sparse.SparseVector;

public class LDAModel {

  public int numTopics;
  public int numTerms;
  public double alpha;
  public double beta;
  public DenseVector gtc;
  public SparseVector[] ttc;
  DenseVector t;
  SparseVector[] w;
  final ThreadLocal localD = new ThreadLocal<DenseVector>();

  public LDAModel(int numTopics, int numTerms, double alpha, double beta) {
    this(new DenseVector(numTopics), new SparseVector[numTerms], alpha, beta);
  }

  public LDAModel(DenseVector gtc, SparseVector[] ttc, double alpha, double beta) {
    this.numTopics = gtc.size();
    this.numTerms = ttc.length;
    this.alpha = alpha;
    this.beta = beta;
    for (int i = 0; i < ttc.length; i++) {
      if (ttc[i] == null) {
        ttc[i] = new SparseVector(numTopics);
      }
    }
    this.gtc = gtc;
    this.ttc = ttc;
  }

  public SparseVector inference(SparseVector doc, int totalIter, int burnIn, Random rand) {
    Preconditions.checkArgument(totalIter > burnIn, "totalIter is less than burnInIter");
    Preconditions.checkArgument(totalIter > 0, "totalIter is less than 0");
    Preconditions.checkArgument(burnIn > 0, "burnInIter is less than 0");

    if (t == null) initCache();
    SparseVector topicDist = new SparseVector(numTopics);
    SparseVector docTopicCounter = uniformDistSampler(doc, rand);

    for (int i = 0; i < totalIter; i++) {
      docTopicCounter = generateTopicDistForDocument(docTopicCounter, doc, rand);
      if (i + burnIn >= totalIter) topicDist.add(docTopicCounter);
    }
    topicDist.scale(1.0 / topicDist.norm(Norm.One));
    topicDist.compact();
    return topicDist;
  }

  SparseVector generateTopicDistForDocument(
      SparseVector docTopicCounter,
      SparseVector doc,
      Random rand) {
    SparseVector newDocTopicCounter = new SparseVector(numTopics);
    Iterator<VectorEntry> iter = doc.iterator();
    while (iter.hasNext()) {
      VectorEntry kv = iter.next();
      int term = kv.index();
      double cn = kv.get();
      int i = 0;
      while (i < cn) {
        int newTopic = termMultinomialDistSampler(docTopicCounter, term, rand);
        newDocTopicCounter.add(newTopic, 1);
        i += 1;
      }
    }
    return newDocTopicCounter;
  }

  int termMultinomialDistSampler(
      SparseVector docTopicCounter,
      int term,
      Random rand) {
    DenseVector d = this.d(docTopicCounter, term);
    return multinomialDistSampler(rand, t, w[term], docTopicCounter, d);
  }


  int multinomialDistSampler(
      Random rand,
      DenseVector t,
      SparseVector w,
      SparseVector docTopicCounter,
      DenseVector d) {
    double lastSum = t.get(numTopics - 1) + w.getData()[w.getUsed() - 1] + d.get(numTopics - 1);
    double distSum = rand.nextDouble() * lastSum;

    int newTopic = binarySearchInterval(t, w, docTopicCounter, d, distSum, 0, numTopics, true);
    return java.lang.Math.min(newTopic, numTopics - 1);

  }

  static double index(
      DenseVector t,
      SparseVector w,
      SparseVector docTopicCounter,
      DenseVector d,
      int i) {
    double lastDS = maxMinD(i, docTopicCounter, d);
    double lastWS = maxMinW(i, w);
    double lastTS = t.get(i);
    return lastDS + lastWS + lastTS;
  }

  static double maxMinD(int i, SparseVector docTopicCounter, DenseVector d) {
    int[] index = docTopicCounter.getIndex();
    int used = docTopicCounter.getUsed();
    int pos = binarySearchInterval(index, i, 0, used, false);
    if (pos > -1)
      return d.get(index[pos]);
    else
      return 0.0;
  }

  static double maxMinW(int i, SparseVector w) {
    int[] index = w.getIndex();
    int used = w.getUsed();
    double[] data = w.getData();
    int pos = binarySearchInterval(index, i, 0, used, false);
    if (pos > -1)
      return data[pos];
    else
      return 0.0;
  }

  DenseVector d(SparseVector docTopicCounter, int term) {
    if (localD.get() == null) localD.set(new DenseVector(numTopics));
    DenseVector d = (DenseVector) localD.get();
    double lastSum = 0D;
    Iterator<VectorEntry> iter = docTopicCounter.iterator();
    while (iter.hasNext()) {
      VectorEntry kv = iter.next();
      int topic = kv.index();
      double cn = kv.get();
      double lastD = cn * (ttc[term].get(topic) + beta) /
          (gtc.get(topic) + (numTerms * beta));
      lastSum += lastD;
      d.set(topic, lastSum);
    }

    d.set(numTopics - 1, lastSum);
    return d;
  }

  SparseVector uniformDistSampler(SparseVector doc, Random rand) {
    SparseVector docTopicCounter = new SparseVector(numTopics);
    double docLen = doc.norm(Norm.One);
    for (int i = 0; i < docLen; i++) {
      docTopicCounter.add(rand.nextInt(numTopics), 1.0);
    }
    return docTopicCounter;
  }

  /**
   * 储存格式
   * 文件第一行储存lda的参数,已空格分开.顺序是
   * numTopics numTerms alpha beta
   * 其他行储存词的主题计数(稀疏储存) 格式是
   * 词Id [主题Id:主题计数 ..]
   * eg:
   * 5000 101262 0.01 0.01
   * 0 18:0.1 1011:6.2 1605:0.1 1761:26.4 2043:0.3 2091:0.1 2093:0.1 2468:0.1 2630:0.2
   * 1 258:0.1 275:0.1 291:0.1 581:0.7 596:0.5 607:0.1 630:0.2 746:0.1 1000:0.2 1003:0.2
   * 2 714:0.1 1941:0.1 2076:0.2 2783:0.1 3086:0.1 3094:0.1 3504:0.1 3613:0.1 3781:0.1
   * 4 258:0.1 503:0.1 581:0.1 618:0.1 701:0.1 718:0.1 745:0.1 763:0.1 1033:2.2
   * 5 395:0.1 887:0.1 1437:0.1 1732:0.1 1871:0.1 2120:0.1 2349:0.1 2915:0.1 3133:0.1 4959:0.1
   */
  public static LDAModel load(String filePath) throws IOException {
    File file = new File(filePath);
    Preconditions.checkArgument(file.exists(), "model file " + filePath +
        " does not exist");
    Preconditions.checkArgument(file.isFile(), "model file " + filePath +
        " is not a normal file");

    List<String> lines = Files.readLines(file, Charsets.UTF_8);
    String[] sp = lines.get(0).split(" ");
    int numTopics = Integer.parseInt(sp[0]);
    int numTerms = Integer.parseInt(sp[1]);
    double alpha = Double.parseDouble(sp[2]);
    double beta = Double.parseDouble(sp[3]);
    LDAModel topicModel = new LDAModel(numTopics, numTerms, alpha, beta);

    Iterator<String> iter = lines.listIterator(1);

    while (iter.hasNext()) {
      String line = iter.next();
      if (line.length() > 1) {
        String[] items = line.split(" ");
        int offset = Integer.parseInt(items[0]);
        for (int i = 1; i < items.length; i++) {
          sp = items[i].split(":");
          int index = Integer.parseInt(sp[0]);
          double value = Double.parseDouble(sp[1]);
          topicModel.merge(offset, index, value);
        }
      }
    }
    return topicModel;
  }

  void merge(int term, int topic, double inc) {
    gtc.add(topic, inc);
    ttc[term].add(topic, inc);
  }

  void initCache() {
    t = null;
    w = null;
    DenseVector t = new DenseVector(numTopics);
    double lastSum = 0D;
    for (int topic = 0; topic < numTopics; topic++) {
      double lastT = alpha * beta / (gtc.get(topic) + (numTerms * beta));
      lastSum += lastT;
      t.set(topic, lastSum);
    }
    this.t = t;
    SparseVector[] w = new SparseVector[numTerms];
    for (int term = 0; term < numTerms; term++) {
      w[term] = new SparseVector(numTopics);
      lastSum = 0D;
      Iterator<VectorEntry> iter = ttc[term].iterator();
      while (iter.hasNext()) {
        VectorEntry kv = iter.next();
        int topic = kv.index();
        double cn = kv.get();
        double lastW = alpha * cn / (gtc.get(topic) + (numTerms * beta));
        lastSum += lastW;
        w[term].set(topic, lastSum);
      }
    }
    this.w = w;
  }

  static int binarySearchInterval(
      int[] index,
      int key,
      int begin,
      int end,
      boolean greater) {

    // Zero length array?
    if (begin == end)
      if (greater)
        return end;
      else
        return begin - 1;

    end--; // Last index
    int mid = (end + begin) >> 1;

    // The usual binary search
    while (begin <= end) {
      mid = (end + begin) >> 1;

      if (index[mid] < key)
        begin = mid + 1;
      else if (index[mid] > key)
        end = mid - 1;
      else
        return mid;
    }

    // No direct match, but an inf/sup was found
    if ((greater && index[mid] >= key) || (!greater && index[mid] <= key))
      return mid;
      // No inf/sup, return at the end of the array
    else if (greater)
      return mid + 1; // One past end
    else
      return mid - 1; // One before start
  }


  private static int binarySearchInterval(
      DenseVector t,
      SparseVector w,
      SparseVector docTopicCounter,
      DenseVector d,
      Double key,
      int begin,
      int end,
      boolean greater) {

    // Zero length array?
    if (begin == end)
      if (greater)
        return end;
      else
        return begin - 1;

    end--; // Last index
    int mid = (end + begin) >> 1;

    // The usual binary search
    while (begin <= end) {
      mid = (end + begin) >> 1;
      double v = index(t, w, docTopicCounter, d, mid);
      if (v < key)
        begin = mid + 1;
      else if (v > key)
        end = mid - 1;
      else
        return mid;
    }

    double v = index(t, w, docTopicCounter, d, mid);
    // No direct match, but an inf/sup was found
    if ((greater && v >= key) || (!greater && v <= key))
      return mid;
      // No inf/sup, return at the end of the array
    else if (greater)
      return mid + 1; // One past end
    else
      return mid - 1; // One before start
  }
}
